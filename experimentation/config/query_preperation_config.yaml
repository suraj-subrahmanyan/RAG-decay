# Model settings
model:
  name: "Qwen/Qwen3-4B-Thinking-2507"
  device: "cuda" 
  torch_dtype: "bfloat16"
  trust_remote_code: true

# Generation parameters
generation:
  temperature: 0.7
  max_new_tokens: 1024
  top_p: 0.9
  do_sample: false

# Input/output files
queries_file: "retrieval_results/queries_answers.backup.jsonl"
backup_suffix: ".qwen.backup.jsonl"